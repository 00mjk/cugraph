{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU PageRank Demo\n",
    "=======================\n",
    "\n",
    "This example runs PageRank on HiBench data using RAPIDS. Before running this notebook, you need to start a DASK scheduler and DASK workers in command line.\n",
    "\n",
    "1) dask-scheduler --scheduler-file [scheduler_file_path]\n",
    "\n",
    "* [scheduler_file_path]: path to write scheduler access information in the json format (e.g. /home/USERID/cluster.json).\n",
    "\n",
    "2) mpirun -np [number_of_workers] --machinefile [machine_address_file_path] dask-mpi --no-nanny --nthreads [number_of_threads] --local-directory [local_directory_path] --no-scheduler --scheduler-file [scheduler_file_path]\n",
    "\n",
    "* [numbrer_of_workers]: number of DASK workers to start, set this to the number of GPUs on your machine.\n",
    "* [machine_address_file_path]: path to the MPI machine address file, --machinefile [machine_address_file_path] can be skipped for single node systems.\n",
    "* [numbrer_of_threads]: number of threads per worker, should be >= 2.\n",
    "* [local_directory_path]: path to place temporary worker files.\n",
    "* [scheduler_file_path]: path to read scheduler access information written by the DASK scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import Files\n",
    "=======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "import dask_cudf\n",
    "import dask_cugraph as dcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Set the Number of GPU Devices and File Paths\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_devices = 2\n",
    "scheduler_file_path = r\"/home/USERID/cluster.json\"\n",
    "input_data_path = r\"/datasets/pagerank/Input-bigdata/edges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define Utility Functions\n",
    "=======================\n",
    "set_visible maps a dask-mpi process to a single GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_visible(i, n):\n",
    "    all_devices = list(range(n))\n",
    "    visible_devices = \",\".join(map(str, all_devices[i:] + all_devices[:i]))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = visible_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create a Client\n",
    "=======================\n",
    "Connect to the DASK scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # start timing from here\n",
    "\n",
    "client = Client(scheduler_file=scheduler_file_path,\n",
    "                direct_to_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Map One Worker to One GPU\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: status: pending, key: set_visible-4f3af2f890d6a36d7e8217c80e27002c>,\n",
       " <Future: status: pending, key: set_visible-734ca7f9b333fe2c93c28d89ff836b50>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices = list(range(number_of_devices))\n",
    "device_workers = list(client.has_what().keys())\n",
    "assert len(devices) == len(device_workers)\n",
    "\n",
    "[client.submit(set_visible, device, len(devices), workers=[worker])\n",
    "    for device, worker in zip(devices, device_workers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Read Input Data\n",
    "=======================\n",
    "Need to replace /datasets/pagerank_demo/Input-bigdata/edges to the proper directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgdf = dask_cudf.read_csv(input_data_path + r\"/part-*\",\n",
    "                          delimiter='\\t', names=['src', 'dst'],\n",
    "                          dtype=['int32', 'int32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Run PageRank\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "pagerank = dcg.mg_pagerank(dgdf)\n",
    "print(pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Close the Client and Report Execution Time\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "\n",
    "end_time = time.time()\n",
    "print((end_time - start_time), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
